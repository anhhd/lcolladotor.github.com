<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Help on L. Collado-Torres</title>
    <link>http://lcolladotor.github.io/tags/help/index.xml</link>
    <description>Recent content in Help on L. Collado-Torres</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2011-2017 Leonardo Collado Torres under (CC) BY-NC-SA</copyright>
    <atom:link href="/tags/help/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Trying to reduce the memory overhead when using mclapply</title>
      <link>http://lcolladotor.github.io/2013/11/14/Trying-to-reduce-the-memory-overhead-when-using-mclapply</link>
      <pubDate>Thu, 14 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>http://lcolladotor.github.io/2013/11/14/Trying-to-reduce-the-memory-overhead-when-using-mclapply</guid>
      <description>

&lt;p&gt;I am currently trying to understand how to reduce the memory used by &lt;code&gt;mclapply&lt;/code&gt;. This function is rather complicated and others have explained the differences versus &lt;code&gt;parLapply&lt;/code&gt; (&lt;span class=&#34;showtooltip&#34; title=&#34;A\_Skelton73 (2013). &#39;understanding the differences between mclapply and parLapply in R.&#39; .&#34;&gt;&lt;a href=&#34;http://stackoverflow.com/questions/17196261/understanding-the-differences-between-mclapply-and-parlapply-in-r&#34;&gt;A_Skelton73, 2013&lt;/a&gt;&lt;/span&gt;; &lt;span class=&#34;showtooltip&#34; title=&#34;lockedoff (2012). &#39;Using mclapply, foreach, or something else in [r] to operate on an object in parallel?&#39; .&#34;&gt;&lt;a href=&#34;http://stackoverflow.com/questions/11036702/using-mclapply-foreach-or-something-else-in-r-to-operate-on-an-object-in-par&#34;&gt;lockedoff, 2012&lt;/a&gt;&lt;/span&gt; ) and also made it clear that in &lt;code&gt;mclapply&lt;/code&gt;  each job does not know if the others are running out of memory and thus cannot trigger &lt;code&gt;gc&lt;/code&gt; (&lt;span class=&#34;showtooltip&#34; title=&#34;(2013). &#39; [R-sig-hpc] mclapply: rm intermediate objects and returning   memory .&#39; .&#34;&gt;&lt;a href=&#34;https://mailman.stat.ethz.ch/pipermail/r-sig-hpc/2012-October/001534.html&#34;&gt;Urbanek, 2012&lt;/a&gt;&lt;/span&gt;).&lt;/p&gt;

&lt;p&gt;While I still struggle to understand all the details of &lt;code&gt;mclapply&lt;/code&gt;, I can successfully use it to reduce computation time at the expense of a very high memory load. I am still looking for tips on how to reduce this memory load.&lt;/p&gt;

&lt;p&gt;Here is what I have done.&lt;/p&gt;

&lt;h2 id=&#34;problem-setting&#34;&gt;Problem setting&lt;/h2&gt;

&lt;p&gt;I have a large data set on the form of a data.frame. I want to apply a function that works using subsets of the data.frame without the need for communication between the chunks, and I want to apply the function fast. In other words, I can safely split the matrix and speed the computation process using &lt;code&gt;mclapply&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;While this works, I would like to minimize memory consumption.&lt;/p&gt;

&lt;h2 id=&#34;toy-data&#34;&gt;Toy data&lt;/h2&gt;

&lt;p&gt;Here is just some toy data for the example.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## The real data set is much larger than this
set.seed(20131113)
data &amp;lt;- data.frame(matrix(rnorm(1e+05), ncol = 10))
dim(data)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 10000    10
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;approach-1&#34;&gt;Approach 1&lt;/h2&gt;

&lt;p&gt;The first approach I have used is to pre-split the data and then use &lt;code&gt;mclapply&lt;/code&gt; over the split data. For illustrative purposes, lets say that the function I want to apply is just &lt;code&gt;rowMeans&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Pre-split the data
dataSplit &amp;lt;- split(data, rep(1:10, each = 1000))

## Approach 1
library(&amp;quot;parallel&amp;quot;)
res1 &amp;lt;- mclapply(dataSplit, rowMeans, mc.cores = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gets the job done, but because my real &lt;code&gt;dataSplit&lt;/code&gt; is much larger in memory, using say 8-10 cores blows up the memory.&lt;/p&gt;

&lt;h3 id=&#34;best-way-to-pre-split&#34;&gt;Best way to pre-split?&lt;/h3&gt;

&lt;p&gt;If I know that if I am using \( n \) number of cores (in this example \( n=2 \) ) and the data set has \( m \) rows, then one option for approach #1 is to split the data into \( n \) chunks each of size \( m / n \) (rounding if needed).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Pre-split the data into m/n chunks
dataSplit1b &amp;lt;- split(data, rep(1:2, each = 5000))

## Approach 1b
res1b &amp;lt;- mclapply(dataSplit1b, rowMeans, mc.cores = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The memory needed is then in part determined by the chunksize (1000 vs 5000 shown above). One excellent suggestion (via Ben) is to reduce the memory load using this approach is to just smaller chunks. However, the runtime of the function I want to apply (&lt;code&gt;rowMeans&lt;/code&gt; in the example) is not very sensible to the chunksize used, thus using very small chunks is not ideal as it increases computation time. Finding the sweet point is tricky, but using chunksizes of \(m / (2n) \) could certainly help memory wise without majorly affecting computation time.&lt;/p&gt;

&lt;h2 id=&#34;approach-2&#34;&gt;Approach 2&lt;/h2&gt;

&lt;p&gt;One suggestion (via Roger) is to use an environment in order to minimize copying (and thus memory load) while using &lt;code&gt;mclapply&lt;/code&gt; over a set of indexes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Save the split data in an environment
my.env &amp;lt;- new.env()
my.env$data1 &amp;lt;- dataSplit1b[[1]]
my.env$data2 &amp;lt;- dataSplit1b[[2]]

## Function that takes indexes, then extracts the data from the environment
applyMyFun &amp;lt;- function(idx, env) {
    eval(parse(text = paste0(&amp;quot;result &amp;lt;- env$&amp;quot;, ls(env)[idx])))
    rowMeans(result)
}

## Approach 2
index &amp;lt;- 1:2
names(index) &amp;lt;- 1:2
res2 &amp;lt;- mclapply(index, applyMyFun, env = my.env, mc.cores = 2)

## Same result?
identical(res1b, res2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;approach-3&#34;&gt;Approach 3&lt;/h2&gt;

&lt;p&gt;Another suggestion (via Roger) is to save the data chunks and load them individually inside the function that I pass to &lt;code&gt;mclapply&lt;/code&gt;. This does not seem ideal in terms of having to create the temporary chunk data files. But I would expect this method to have the lowest memory footprint.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## Save the chunks
for (i in names(dataSplit1b)) {
    chunk &amp;lt;- dataSplit1b[[i]]
    output &amp;lt;- paste0(&amp;quot;chunk&amp;quot;, i, &amp;quot;.Rdata&amp;quot;)
    save(chunk, file = output)
}

## Function that loads the chunk
applyMyFun2 &amp;lt;- function(idx) {
    load(paste0(&amp;quot;chunk&amp;quot;, idx, &amp;quot;.Rdata&amp;quot;))
    rowMeans(chunk)
}

## Approach 3
res3 &amp;lt;- mclapply(index, applyMyFun2, mc.cores = 2)

## Same result?
identical(res1b, res3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computation-time-comparison&#34;&gt;Computation time comparison&lt;/h2&gt;

&lt;p&gt;Computation time wise, approaches 2 and 3 do not seem very different. Approach 1b seems a tiny bit faster. [Edit: the order of the best approach might change slightly if you re-run this code]&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;microbenchmark&amp;quot;)
micro &amp;lt;- microbenchmark(mclapply(dataSplit1b, rowMeans, mc.cores = 2), mclapply(index, 
    applyMyFun, env = my.env, mc.cores = 2), mclapply(index, applyMyFun2, mc.cores = 2))
micro
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##                                                     expr   min    lq
##            mclapply(dataSplit1b, rowMeans, mc.cores = 2) 17.43 19.97
##  mclapply(index, applyMyFun, env = my.env, mc.cores = 2) 17.05 19.20
##               mclapply(index, applyMyFun2, mc.cores = 2) 17.19 23.11
##  median    uq   max neval
##   21.41 26.00 65.53   100
##   20.60 23.92 43.67   100
##   24.56 28.39 46.99   100
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(&amp;quot;ggplot2&amp;quot;)
autoplot(micro)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://lcolladotor.github.io/figs/2013-11-14-Reducing-memory-overhead-when-using-mclapply/compTime.png&#34; alt=&#34;center&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;memory-wise-comparison&#34;&gt;Memory wise comparison&lt;/h2&gt;

&lt;p&gt;Relying on the cluster tools for calculating the maximum memory used, I ran each approach (1b, 2, and 3) ten times each using 2 cores using the scripts available in &lt;a href=&#34;https://gist.github.com/lcolladotor/7462753&#34; target=&#34;_blank&#34;&gt;this gist&lt;/a&gt;. The maximum memory used showed no variability (within an approach) and the results are that approach 1b used 1.224G RAM, approach 2 used 1.176G RAM, and approach 3 used 1.177G RAM. Not a huge difference. Due to having to write and then load, approach 3 was slower than the other two.&lt;/p&gt;

&lt;p&gt;Re-doing the previous test but using 20 cores lead to very similar wall clock computation times between all three approaches and to approaches 1b and 2 for 2 cores. This is due to the nature of the example, aka &lt;code&gt;rowMeans&lt;/code&gt; is fast even with the larger chunks. Approach 1b used 7.728G RAM, approach 2 used 7.674G RAM, and approach 3 used 7.690G RAM. Hm&amp;hellip;&lt;/p&gt;

&lt;p&gt;Using 20 cores with previously created data files (either the split data for approaches 1b and 2, or the chunk files for approach 3) has a very different memory footprint. Approach 1b used in average 6.0744G RAM, approach 2 used 4.2647G RAM
, and approach 3 used 2.6545G RAM.&lt;/p&gt;

&lt;h3 id=&#34;edit&#34;&gt;Edit&lt;/h3&gt;

&lt;p&gt;Ryan from (&lt;span class=&#34;showtooltip&#34; title=&#34;(2013). &#39; [Bioc-devel] Trying to reduce the memory overhead when using mclapply .&#39; .&#34;&gt;&lt;a href=&#34;https://stat.ethz.ch/pipermail/bioc-devel/2013-November/004930.html&#34;&gt;Ryan 2013&lt;/a&gt;&lt;/span&gt;) contributed a fourth approach which used 6.794G RAM when starting from scratch with 20 cores. This approach definitely beats the other ones under the condition of starting from scratch. Note that just creating the &lt;code&gt;data&lt;/code&gt; object uses 558.938M RAM: multiplied by 20 it would be around 10.92G RAM.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Using 2 or 20 cores, approach 2 beat by a very small margin approaches 3 and 1b in terms of memory usage. However, all approaches failed in terms of not having the memory blow up as you increase the number of cores when starting from scratch.&lt;/p&gt;

&lt;p&gt;If a lower memory option is used for splitting the data and creating the chunk files, approach 3 seems like the winner in terms of memory usage. So in pure terms of lowering the memory load on &lt;code&gt;mclapply&lt;/code&gt; approach 3 wins, although you still need to create the chunk files and do so without much memory usage.&lt;/p&gt;

&lt;p&gt;If you have any ideas or suggestions, please let me know! Thank you!&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;Citations made with &lt;code&gt;knitcitations&lt;/code&gt; (&lt;span class=&#34;showtooltip&#34; title=&#34;Boettiger C (2013). knitcitations: Citations for knitr markdown files. R package version 0.4-7.&#34;&gt;&lt;a href=&#34;http://CRAN.R-project.org/package=knitcitations&#34;&gt;Boettiger, 2013&lt;/a&gt;&lt;/span&gt;).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A_Skelton73,   (2013) understanding the differences between mclapply and parLapply in R.  &lt;em&gt;understanding the differences between mclapply and parLapply in R - Stack Overflow&lt;/em&gt;  &lt;a href=&#34;http://stackoverflow.com/questions/17196261/understanding-the-differences-between-mclapply-and-parlapply-in-r&#34; target=&#34;_blank&#34;&gt;http://stackoverflow.com/questions/17196261/understanding-the-differences-between-mclapply-and-parlapply-in-r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;lockedoff,   (2012) Using mclapply, foreach, or something else in [r] to operate on an object in parallel?.  &lt;em&gt;Using mclapply, foreach, or something else in [r] to operate on an object in parallel? - Stack Overflow&lt;/em&gt;  &lt;a href=&#34;http://stackoverflow.com/questions/11036702/using-mclapply-foreach-or-something-else-in-r-to-operate-on-an-object-in-par&#34; target=&#34;_blank&#34;&gt;http://stackoverflow.com/questions/11036702/using-mclapply-foreach-or-something-else-in-r-to-operate-on-an-object-in-par&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[R-sig-hpc] mclapply: rm intermediate objects and returning memory
.  &lt;a href=&#34;https://mailman.stat.ethz.ch/pipermail/r-sig-hpc/2012-October/001534.html&#34; target=&#34;_blank&#34;&gt;https://mailman.stat.ethz.ch/pipermail/r-sig-hpc/2012-October/001534.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Bioc-devel] Trying to reduce the memory overhead when using mclapply
.  &lt;a href=&#34;https://stat.ethz.ch/pipermail/bioc-devel/2013-November/004930.html&#34; target=&#34;_blank&#34;&gt;https://stat.ethz.ch/pipermail/bioc-devel/2013-November/004930.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Carl Boettiger,   (2013) knitcitations: Citations for knitr markdown files.  &lt;a href=&#34;http://CRAN.R-project.org/package=knitcitations&#34; target=&#34;_blank&#34;&gt;http://CRAN.R-project.org/package=knitcitations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;reproducibility&#34;&gt;Reproducibility&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## R version 3.0.2 (2013-09-25)
## Platform: x86_64-apple-darwin10.8.0 (64-bit)
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
## [1] microbenchmark_1.3-0 ggplot2_0.9.3.1      knitcitations_0.4-7 
## [4] bibtex_0.3-6         knitr_1.5           
## 
## loaded via a namespace (and not attached):
##  [1] codetools_0.2-8    colorspace_1.2-4   dichromat_2.0-0   
##  [4] digest_0.6.4       evaluate_0.5.1     formatR_0.10      
##  [7] grid_3.0.2         gtable_0.1.2       httr_0.2          
## [10] labeling_0.2       MASS_7.3-29        munsell_0.4.2     
## [13] plyr_1.8           proto_0.3-10       RColorBrewer_1.0-5
## [16] RCurl_1.95-4.1     reshape2_1.2.2     scales_0.2.3      
## [19] stringr_0.6.2      tools_3.0.2        XML_3.95-0.2      
## [22] xtable_1.7-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scripts&#34;&gt;Scripts&lt;/h3&gt;

&lt;p&gt;The scripts are available in &lt;a href=&#34;https://gist.github.com/lcolladotor/7462753&#34; target=&#34;_blank&#34;&gt;this gist&lt;/a&gt;. The main one is &lt;code&gt;testApproach.R&lt;/code&gt; while the other ones are just job-submitters.&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/lcolladotor/7462753.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Check other topics on &lt;a href=&#34;https://twitter.com/search?q=%23rstats&#34; target=&#34;_blank&#34;&gt;#rstats&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
